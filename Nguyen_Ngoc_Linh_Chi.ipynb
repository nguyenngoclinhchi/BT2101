{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vhnFCTTmlBWp"
   },
   "source": [
    "## BT2101 Group Project \n",
    "\n",
    "### Author: Nguyen Ngoc Linh Chi - Matric : A0170767W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100
    },
    "colab_type": "code",
    "id": "mdTi-xBost_Q",
    "outputId": "daa07d50-f5e1-42f9-ecd9-b52983b0ddce"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------GOOGLE COLAB UTILITY- ------------------------------------------------------#\n",
    "\n",
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    " process = psutil.Process(os.getpid())\n",
    " print(\"Gen RAM Free: \" \n",
    "       + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" \n",
    "       + humanize.naturalsize( process.memory_info().rss))\n",
    " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, \n",
    "                                                                                             gpu.memoryUtil*100, \n",
    "                                                                                             gpu.memoryTotal))\n",
    "printm() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqB7NvXxss6p"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------MASS IMPORT LIBRARIES ------------------------------------------------------#\n",
    "# import essential libraries for lading data\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 10000)\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# for preprocessing the data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Feature selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Splitting dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Evaluation\n",
    "from scipy.stats import iqr\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_validate\n",
    "\n",
    "## Import confusion_matrix, classification_report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, auc, accuracy_score\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Model\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "\n",
    "# for combining the preprocess with model training\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "\n",
    "# for optimizing the hyperparameters of the pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_validate, RandomizedSearchCV\n",
    "\n",
    "# neural network\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "from keras.models import load_model\n",
    "\n",
    "import keras.backend as K\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "qVJwIzFTswb-",
    "outputId": "437a5bbd-2896-41af-e3ec-8d0f4d58d661"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_cblFAmgem6K"
   },
   "source": [
    "# Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SxuUJNQ4em6L"
   },
   "outputs": [],
   "source": [
    "version_code = \"_NB_\"\n",
    "\n",
    "# class_weight can be None or Balanced\n",
    "class_weight_options = ['None', 'Balanced']\n",
    "\n",
    "# model_type can be Original or Pipeline\n",
    "models_type_options = ['Original', 'Pipeline']\n",
    "\n",
    "# model options\n",
    "model_option = [['None', 'Original']]\n",
    "\n",
    "# Choose whether you are using colab or your own laptop\n",
    "setting = 'colab'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFL-AzTwem6I"
   },
   "source": [
    "# Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5mggb3rSem6I"
   },
   "outputs": [],
   "source": [
    "link = 'https://raw.githubusercontent.com/nguyenngoclinhchi/CS3244-Project/master/BT2101Group-Project/card_filter.csv'\n",
    "\n",
    "def get_paths(mode): \n",
    "    if mode == 'colab': \n",
    "        # -------------------------------------------------GOOGLE COLAB ROOTPATH ----------------------------------------------#\n",
    "        ROOT_PATH = '/content/gdrive/My Drive/BT2101_LC_Private'\n",
    "        ROOT_PATH_MODELS = '/content/gdrive/My Drive/BT2101_LC_Private/models'\n",
    "        ROOT_PATH_SUBMODELS = '/content/gdrive/My Drive/BT2101_LC_Private/models/submodels'\n",
    "    else:\n",
    "        # -------------------------------------------------LAPTOP ROOTPATH ----------------------------------------------------#\n",
    "        ROOT_PATH = '.'\n",
    "        ROOT_PATH_MODELS = './models'\n",
    "        ROOT_PATH_SUBMODELS = './models/submodels'\n",
    "    return ROOT_PATH, ROOT_PATH_MODELS, ROOT_PATH_SUBMODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U4DYpThass6w"
   },
   "source": [
    "# Load dataset from Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wWsV26yZss6x"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(link, index_col='ID')\n",
    "\n",
    "# rename last column\n",
    "data.rename(columns={\"default payment next month\": \"default\"}, inplace=True)\n",
    "\n",
    "X = data.iloc[:,0:23] # input will take from column 1 to 23\n",
    "y = data.iloc[:,-1] # Last column is output\n",
    "\n",
    "# X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cK5MOvP-ss60"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "gWo_57wGss61",
    "outputId": "1d5f0c00-6d8d-4203-8ad5-f70e210ef9e3"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "colab_type": "code",
    "id": "ZjGMAzfTss65",
    "outputId": "718b0bc6-ae6e-4675-dc0f-4d8d4fff28df"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    },
    "colab_type": "code",
    "id": "K2OY8EOgss68",
    "outputId": "d3a5ba52-7af8-471e-83a2-d805b2bbf8d5"
   },
   "outputs": [],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6n6Twufss6-"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "\n",
    "# Plot figures\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "mpl.rcParams['figure.dpi']= 250\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w9AMJ0V5ss7B"
   },
   "source": [
    "## Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "Vsm_HsYnss7C",
    "outputId": "214acc22-8da0-4159-836f-f34accdc15b1"
   },
   "outputs": [],
   "source": [
    "def heatmap(x, y, size, fig_size):\n",
    "    fig, ax = plt.subplots(figsize=(fig_size, fig_size))\n",
    "    \n",
    "    # Mapping from column names to integer coordinates\n",
    "    x_labels = [v for v in sorted(x.unique())]\n",
    "    y_labels = [v for v in sorted(y.unique())]\n",
    "    x_to_num = {p[1]:p[0] for p in enumerate(x_labels)} \n",
    "    y_to_num = {p[1]:p[0] for p in enumerate(y_labels)} \n",
    "    \n",
    "    size_scale = 500\n",
    "    ax.scatter(\n",
    "        x=x.map(x_to_num), # Use mapping for x\n",
    "        y=y.map(y_to_num), # Use mapping for y\n",
    "        s=size * size_scale, # Vector of square sizes, proportional to size parameter\n",
    "        marker='s' # Use square as scatterplot marker\n",
    "    )\n",
    "    \n",
    "    # Show column labels on the axes\n",
    "    ax.set_xticks([x_to_num[v] for v in x_labels])\n",
    "    ax.set_xticklabels(x_labels, rotation=60, horizontalalignment='right')\n",
    "    ax.set_yticks([y_to_num[v] for v in y_labels])\n",
    "    ax.set_yticklabels(y_labels)\n",
    "\n",
    "corr = X.corr()\n",
    "corr = pd.melt(corr.reset_index(), id_vars='index')\n",
    "corr\n",
    "corr.columns = ['x', 'y', 'value']\n",
    "heatmap(\n",
    "    x=corr['x'],\n",
    "    y=corr['y'],\n",
    "    size=corr['value'].abs(), fig_size = 15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "cINR3QDuss7E",
    "outputId": "3aa1d8b4-45b1-411a-8b60-751ca2e0ba6e"
   },
   "outputs": [],
   "source": [
    "corr = X.corr()\n",
    "plt.figure(figsize=(18, 18))\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    annot=True,\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=60,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0iUE-U1iss7H"
   },
   "source": [
    "## Distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "colab_type": "code",
    "id": "cTfge-_Bss7I",
    "outputId": "b5cd8494-4116-4e6c-e6e4-5e4745e5d533"
   },
   "outputs": [],
   "source": [
    "train = X.join(y)\n",
    "df = train\n",
    "data_features = df.iloc[:,0:23].columns\n",
    "print(data_features.__len__())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "cNz8a1--ss7K",
    "outputId": "4e87e6e5-1bf8-4ca4-aa62-3906d4bbffc8"
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize=(12,23*4))\n",
    "gs = mpl.gridspec.GridSpec(23, 1)\n",
    "for i, cn in enumerate(df[data_features]):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    sns.distplot(df[cn][df.default == 1], bins=50)\n",
    "    sns.distplot(df[cn][df.default == 0], bins=50)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title('histogram of feature: ' + str(cn))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TLuuTlo2ss7N"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MfRmeTkJss7O"
   },
   "outputs": [],
   "source": [
    "def preprocess_input(X):\n",
    "    # Turn age feature into log(age)\n",
    "    if (\"log10(age+1)\" not in X.columns) and (\"age\" in X.columns):\n",
    "        X[\"log10(age+1)\"] = X[\"age\"].transform(lambda x: np.log10(x + 1))\n",
    "        X.drop(\"age\", axis = 1, inplace = True)\n",
    "    # Drop unnecessary features\n",
    "    dropped_features = [\n",
    "        \"SEX\", \"PAY_5\", \"MARRIAGE\", \"PAY_3\", \"EDUCATION\", \"PAY_2\", \"PAY_4\", \"PAY_6\"\n",
    "    ]\n",
    "    for feature in dropped_features:\n",
    "        if feature in X.columns:\n",
    "            X.drop(feature, axis = 1, inplace = True)\n",
    "    # Turn all category features into multiple one-hot features\n",
    "    scaler = MinMaxScaler(feature_range=(0,1)).fit(X)\n",
    "    data_rescaled = scaler.fit_transform(X)\n",
    "    data_rescaled_df = pd.DataFrame(data_rescaled)\n",
    "    data_rescaled_df.describe()\n",
    "    data_rescaled_df.columns = X.columns\n",
    "    X = data_rescaled_df\n",
    "    return X\n",
    "\n",
    "X = preprocess_input(X.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J7C6nAiIss7S"
   },
   "outputs": [],
   "source": [
    "X_ori = X # save X before using scaler\n",
    "y_ori = y # save y before using scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H95BMjnzss7V"
   },
   "source": [
    "## Principle Analysis Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsxOVD4Xss7W"
   },
   "outputs": [],
   "source": [
    "# Decide what n_components for PCA\n",
    "## Fitting the PCA algorithm with our Data\n",
    "pca = PCA().fit(X)\n",
    "\n",
    "for i in range(np.cumsum(pca.explained_variance_ratio_).__len__()): \n",
    "    print(\"n_component = \" + str(i + 1) + \" --> \" + str(np.cumsum(pca.explained_variance_ratio_)[i]))\n",
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Card Dataset Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KmFwAxnDss7a"
   },
   "outputs": [],
   "source": [
    "## Based on the above graph --> Choose n = 8\n",
    "n_component = 8\n",
    "arr = []\n",
    "for i in range(n_component): \n",
    "    arr.append('#'+ str(i+ 1))\n",
    "pca = PCA(n_components=n_component)\n",
    "principalComponents = pca.fit_transform(X.values)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = arr)\n",
    "finalDf = pd.concat([principalDf, y], axis = 1)\n",
    "finalDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n6TUzuP9ss7f"
   },
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-2Q1AYfss7g"
   },
   "outputs": [],
   "source": [
    "# Univariate selection \n",
    "## apply SelectKBest class to extract features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=X.columns.__len__())\n",
    "\n",
    "fit = bestfeatures.fit(X,y)\n",
    "\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(X.__len__(),'Score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tTtgSj-Iss7j"
   },
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "\n",
    "model.fit(X,y)\n",
    "\n",
    "print(model.feature_importances_, X.columns) #use inbuilt class feature_importances of tree based classifiers\n",
    "\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(X.__len__()).plot(kind='barh')\n",
    "plt.figure(figsize=(18, 18))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jqOsqymsss7m"
   },
   "source": [
    "# Splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6dc4IgVss7n"
   },
   "outputs": [],
   "source": [
    "#START TRAINING DATA\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "asOueP4M4AiN"
   },
   "outputs": [],
   "source": [
    "#START TRAINING DATA\n",
    "X_train_ori, X_test_ori, y_train_ori, y_test_ori = train_test_split(X_ori, y_ori, test_size = 0.30, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5bnncJKss7u"
   },
   "source": [
    "## Resampling treatement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZhjpun32YFa"
   },
   "outputs": [],
   "source": [
    "def create_meta_data(X_train, y_train): \n",
    "    training_data = pd.concat ([X_train,y_train],axis = 1)\n",
    "\n",
    "    fig = plt.figure(figsize=(5,5)) # Set figsize\n",
    "    sns.countplot(data=training_data, x='default')\n",
    "    plt.show()\n",
    "    ############################################################################################################################\n",
    "    #                                              RESAMPLING MANUAL                                                           #\n",
    "    ############################################################################################################################\n",
    "    # Check default payment next month\n",
    "    percentage_pay = (data['default'] == 1).sum() / data.shape[0] * 100\n",
    "    percentage_no_pay = (data['default'] == 0).sum() / data.shape[0] * 100\n",
    "\n",
    "    print ('default payment next month = 1 : ', percentage_pay)\n",
    "    print ('default payment next month = 0: ', percentage_no_pay)\n",
    "\n",
    "\n",
    "    print ('Percentage original pay: ', percentage_pay)\n",
    "    print ('Percentage original no-pay: ', percentage_no_pay)\n",
    "    number_of_instances = 30000\n",
    "\n",
    "    number_sub_pay = int (percentage_pay/100 * number_of_instances)\n",
    "    number_sub_non_pay = int (percentage_no_pay/100 * number_of_instances)\n",
    "\n",
    "    sub_pay_data = training_data[training_data['default'] == 1].head(number_sub_pay)\n",
    "    sub_non_pay_data = training_data[training_data['default'] == 0].head(number_sub_non_pay)\n",
    "\n",
    "    print ('Number of newly sub pay data:',len(sub_pay_data))\n",
    "    print ('Number of newly sub non-pay data:',len(sub_non_pay_data))\n",
    "\n",
    "    sub_training_data = pd.concat ([sub_pay_data, sub_non_pay_data], axis = 0)\n",
    "    sub_training_data['default'].value_counts()\n",
    "\n",
    "    ############################################################################################################################\n",
    "    #                                              UNDER SAMPLING                                                              #\n",
    "    ############################################################################################################################\n",
    "    # pay/non-pay data\n",
    "    # Select row which \"default payment next month\" is 1 and save in pay_data\n",
    "    pay_data = sub_training_data[sub_training_data['default'] == 1]\n",
    "    # Select row which \"default payment next month\" is 0 and save in non_pay_data\n",
    "    non_pay_data = sub_training_data[sub_training_data['default'] == 0]\n",
    "\n",
    "    # Number of pay, non-pay transactions\n",
    "    number_records_pay = pay_data.shape[0]\n",
    "    number_records_non_pay = non_pay_data.shape[0]\n",
    "\n",
    "    # Using sample function on data frame to randomly select number_records_pay from non_pay_data data frame\n",
    "    under_sample_non_pay = non_pay_data.sample(number_records_pay)\n",
    "    # **concat** under_sample_non_pay and pay_data to form under_sample_data\n",
    "    under_sample_data = pd.concat([under_sample_non_pay, pay_data], axis=0)\n",
    "\n",
    "    under_sample_data.dropna(inplace=True)\n",
    "\n",
    "    # Showing ratio\n",
    "    print(\"Percentage of non pay transactions: \", under_sample_non_pay.shape[0] / under_sample_data.shape[0])\n",
    "    print(\"Percentage of pay transactions: \", pay_data.shape[0] / under_sample_data.shape[0])\n",
    "    print(\"Total number of transactions in resampled data: \", under_sample_data.shape[0])\n",
    "\n",
    "    # Assigning X,y for Under-sampled Data\n",
    "    X_train_undersample = under_sample_data.drop(columns=['default'])\n",
    "    y_train_undersample = under_sample_data['default']\n",
    "\n",
    "    # Plot countplot\n",
    "    plt.figure(figsize=(5,5))\n",
    "    # Make a count plot to show ratio between 2 default payment next month on \"default payment next month\" column\n",
    "    sns.countplot(data=under_sample_data, x='default')\n",
    "    plt.show()\n",
    "\n",
    "    ############################################################################################################################\n",
    "    #                                              OVER SAMPLING                                                               #\n",
    "    ############################################################################################################################\n",
    "    # pay/non-pay data\n",
    "    # Select row which \"default payment next month\" is 1 and save in pay_data\n",
    "    pay_data = sub_training_data[sub_training_data['default'] == 1]\n",
    "    # Select row which \"default payment next month\" is 0 and save in non_pay_data\n",
    "    non_pay_data = sub_training_data[sub_training_data['default'] == 0]\n",
    "\n",
    "    # Number of pay, non-pay transactions\n",
    "    number_records_pay = pay_data.shape[0]\n",
    "    number_records_non_pay = non_pay_data.shape[0]\n",
    "\n",
    "    # Using sample on pay_data with replacement \"replace = True\",  since we take a larger sample than population\n",
    "    over_sample_pay = pay_data.sample(replace = True, n=number_records_non_pay)\n",
    "    # **concat** over_sample_pay and non_pay_data to form under_sample_data\n",
    "    over_sample_data = pd.concat([over_sample_pay, non_pay_data], axis=0)\n",
    "    over_sample_data.dropna(inplace=True)\n",
    "\n",
    "    # Showing ratio\n",
    "    print(\"Percentage of non pay transactions: \", non_pay_data.shape[0]/over_sample_data.shape[0])\n",
    "    print(\"Percentage of pay transactions: \", pay_data.shape[0]/over_sample_data.shape[0])\n",
    "    print(\"Total number of transactions in resampled data: \", over_sample_data.shape[0])\n",
    "\n",
    "    # Assigning X, y for over-sampled dataset\n",
    "    X_train_oversample = over_sample_data.drop(columns=['default'])\n",
    "    y_train_oversample = over_sample_data['default']\n",
    "\n",
    "    # Plot countplot\n",
    "    plt.figure(figsize=(5,5))\n",
    "    # Make a count plot to show ratio between 2 default payment next month on \"default payment next month\" column\n",
    "    sns.countplot(data=over_sample_data, x='default')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    ############################################################################################################################\n",
    "    #                                              SMOTE                                                                       #\n",
    "    ############################################################################################################################\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    sm = SMOTE()\n",
    "    X_smote, y_smote = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    ############################################################################################################################\n",
    "    #                                              ADASYN                                                                      #\n",
    "    ############################################################################################################################\n",
    "    from imblearn.over_sampling import ADASYN \n",
    "    adasyn = ADASYN()\n",
    "    X_ada, y_ada = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "    ############################################################################################################################\n",
    "    #                                              RANDOM UNDER SAMPLER                                                        #\n",
    "    ############################################################################################################################\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    rus = RandomUnderSampler()\n",
    "    X_rus, y_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "    ############################################################################################################################\n",
    "    #                                              Combination: SMOTE + ENN                                                    #\n",
    "    ############################################################################################################################\n",
    "    from imblearn.combine import SMOTEENN\n",
    "    smenn = SMOTEENN()\n",
    "    X_smenn, y_smenn = smenn.fit_resample(X_train, y_train)\n",
    "\n",
    "    ############################################################################################################################\n",
    "    #                                              Returns dictionary of different dataset                                     #\n",
    "    ############################################################################################################################\n",
    "    meta_data = {\n",
    "        'Origin': [X_train, y_train],\n",
    "        'Over sample': [X_train_oversample, y_train_oversample], \n",
    "        'Under sample': [X_train_undersample, y_train_undersample], \n",
    "        'ADASYN': [X_ada, y_ada],\n",
    "        'SMOTE': [X_smote, y_smote], \n",
    "        'SMOTE + ENN': [X_smenn, y_smenn],\n",
    "        'Random under sampler': [X_rus, y_rus],\n",
    "    }\n",
    "    print(\"--------------Meta-Data created-------------------\")\n",
    "    return meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "La2HS_ubss8T"
   },
   "source": [
    "# Evaluate and train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9G0L4A7eem67"
   },
   "outputs": [],
   "source": [
    "ROOT_PATH, ROOT_PATH_MODELS, ROOT_PATH_SUBMODELS = get_paths(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ZkkZE8Pss8d"
   },
   "outputs": [],
   "source": [
    "def get_all_models_lists(model_type, class_weight):\n",
    "\n",
    "    weight_code = 'balanced' if class_weight == 'Balanced' else None\n",
    "\n",
    "    ############################################################################################################################\n",
    "    #                                              LinearSVC                                                                   #\n",
    "    ############################################################################################################################\n",
    "    scaler = MinMaxScaler()\n",
    "    model_SVC = LinearSVC(dual=False, multi_class='ovr', max_iter=10000, class_weight = weight_code)\n",
    "    pipe_SVC = make_pipeline(scaler, model_SVC)\n",
    "\n",
    "    pipe_steps_SVC = list(pipe_SVC.named_steps.keys())\n",
    "    print('Pipeline steps:', pipe_steps_SVC)\n",
    "    model_step_SVC = pipe_steps_SVC[1]\n",
    "\n",
    "    parameters_SVC = [{\n",
    "        model_step_SVC + '__' + 'penalty': ['l1', 'l2'], \n",
    "        model_step_SVC + '__' + 'loss': ['squared_hinge'],\n",
    "        model_step_SVC + '__' + 'tol': [0.0001],\n",
    "        model_step_SVC + '__' + 'C': [1]\n",
    "    }]\n",
    "    gscv_SVC = GridSearchCV(\n",
    "        estimator=pipe_SVC, param_grid=parameters_SVC, scoring=\"accuracy\", cv=5,\n",
    "        n_jobs=-1, verbose=2, return_train_score=True\n",
    "    )\n",
    "\n",
    "    ############################################################################################################################\n",
    "    #                                              SVM WITH RBF                                                                #\n",
    "    ############################################################################################################################\n",
    "    scaler = MinMaxScaler()\n",
    "    pca = PCA()\n",
    "    model_SVM = SVC(kernel=\"rbf\", class_weight = weight_code)\n",
    "    pipe_SVM = make_pipeline(scaler, pca, model_SVM)\n",
    "\n",
    "    pipe_steps_SVM = list(pipe_SVM.named_steps.keys())\n",
    "    print('Pipeline steps:', pipe_steps_SVM)\n",
    "    model_step_PCA = pipe_steps_SVM[1]\n",
    "    model_step_SVM = pipe_steps_SVM[2]\n",
    "    parameters_SVM = {\n",
    "        model_step_PCA + '__n_components': [8],\n",
    "        model_step_SVM + '__C': [0.1, 0.5, 1, 10, 100, 200],\n",
    "        model_step_SVM + '__gamma': [0.001, 0.01, 0.1, 1, 10, 50],\n",
    "    }\n",
    "\n",
    "    gscv_SVM = GridSearchCV(\n",
    "        estimator=pipe_SVM, param_grid=parameters_SVM, scoring=\"accuracy\", cv=5,\n",
    "        n_jobs=-1, verbose=2, return_train_score=True\n",
    "    )\n",
    "    \n",
    "    ############################################################################################################################\n",
    "    #                                              LogisticRegression                                                          #\n",
    "    ############################################################################################################################\n",
    "    scaler = MinMaxScaler()\n",
    "    model_LR = LogisticRegression(solver=\"saga\", multi_class=\"ovr\", max_iter=10000, tol=0.001, class_weight = weight_code)\n",
    "    pipe_LR = make_pipeline(scaler, model_LR)\n",
    "\n",
    "    pipe_steps_LR = list(pipe_LR.named_steps.keys())\n",
    "    print('Pipeline steps:', pipe_steps_LR)\n",
    "    model_step_LR = pipe_steps_LR[1]\n",
    "\n",
    "    parameters_LR = [{\n",
    "        model_step_LR + '__' + 'penalty': [\"l2\"],\n",
    "        model_step_LR + '__' + 'C': [10, 100]\n",
    "    }]\n",
    "\n",
    "    gscv_LR = GridSearchCV(\n",
    "        estimator=pipe_LR, param_grid=parameters_LR, scoring=\"accuracy\", cv=5,\n",
    "        n_jobs=-1, verbose=2, return_train_score=True\n",
    "    )\n",
    "\n",
    "\n",
    "    ############################################################################################################################\n",
    "    #                                              RF WITH RSCV                                                                #\n",
    "    ############################################################################################################################\n",
    "    pipe_RF = make_pipeline(StandardScaler(), RandomForestClassifier(42, class_weight = weight_code))\n",
    "\n",
    "    pipe_steps_RF = list(pipe_RF.named_steps.keys())\n",
    "    print('Pipeline steps:', pipe_steps_RF)\n",
    "    model_step_RF = pipe_steps_RF[1]\n",
    "\n",
    "\n",
    "    #hyperparameter tuning\n",
    "    hp_space = {model_step_RF + '__max_depth': np.arange(30,50),\n",
    "                model_step_RF + '__min_samples_split': np.linspace(0.1, 1.0, 10, endpoint=True),\n",
    "                model_step_RF + '__min_samples_leaf': np.arange(1,10),\n",
    "                model_step_RF + '__max_features': list(range(1,len(X_train.columns))),\n",
    "                model_step_RF + '__n_estimators': [16, 32, 50, 100]\n",
    "    }\n",
    "    randcv = RandomizedSearchCV(\n",
    "        pipe_RF, hp_space, cv=5, verbose=2, scoring='f1_micro', \n",
    "        n_iter=80, n_jobs=-1, return_train_score=True\n",
    "    )\n",
    "\n",
    "    ############################################################################################################################\n",
    "    #                                              RF WITH GSCV                                                                #\n",
    "    ############################################################################################################################\n",
    "    hp_space2 = {'randomforestclassifier__max_depth': [34, 40],\n",
    "                'randomforestclassifier__min_samples_split': [0.1, 0.2, 0.3],\n",
    "                'randomforestclassifier__min_samples_leaf': [3, 4],\n",
    "                'randomforestclassifier__max_features': [X_train.columns.__len__() - 10, \n",
    "                                                         X_train.columns.__len__() - 5, \n",
    "                                                         X_train.columns.__len__()],\n",
    "                'randomforestclassifier__n_estimators': [60, 128]\n",
    "    }\n",
    "\n",
    "    gscv2 = GridSearchCV(pipe_RF, hp_space2, cv=5, verbose=2, scoring='f1_micro', \n",
    "                         n_jobs=-1, return_train_score=True)\n",
    "\n",
    "    ############################################################################################################################\n",
    "    #                                              SGDClassifier                                                               #\n",
    "    ############################################################################################################################\n",
    "    scaler = MinMaxScaler()\n",
    "    model_SGD = SGDClassifier(max_iter=10000, class_weight = weight_code)\n",
    "    pipe_SGD = make_pipeline(scaler, model_SGD)\n",
    "\n",
    "    pipe_steps_SGD = list(pipe_SGD.named_steps.keys())\n",
    "    print('Pipeline steps:', pipe_steps_SGD)\n",
    "    model_step_SGD = pipe_steps_SGD[1]\n",
    "\n",
    "    parameters_SGD = [\n",
    "        {\n",
    "            model_step_SGD + '__' + 'loss': ['hinge', 'log'],\n",
    "            model_step_SGD + '__' + 'penalty': ['none', 'l2', 'l1'],\n",
    "            model_step_SGD + '__' + 'learning_rate': ['optimal'],\n",
    "            model_step_SGD + '__' + 'alpha': [0.0001]\n",
    "        },\n",
    "        {\n",
    "            model_step_SGD + '__' + 'loss': ['hinge', 'log'],\n",
    "            model_step_SGD + '__' + 'penalty': ['l2', 'l1'],\n",
    "            model_step_SGD + '__' + 'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "            model_step_SGD + '__' + 'alpha': [0.0001],\n",
    "            model_step_SGD + '__' + 'eta0': [0.1]\n",
    "        },\n",
    "        {\n",
    "            model_step_SGD + '__' + 'loss': ['hinge', 'log'],\n",
    "            model_step_SGD + '__' + 'penalty': ['none'],\n",
    "            model_step_SGD + '__' + 'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "            model_step_SGD + '__' + 'eta0': [0.1]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    gscv_SGD = GridSearchCV(\n",
    "        estimator=pipe_SGD, param_grid=parameters_SGD, scoring=\"accuracy\", cv=5,\n",
    "        n_jobs=-1, verbose=2, return_train_score=True\n",
    "    )\n",
    "\n",
    "    ############################################################################################################################\n",
    "    #                                              XGBClassifier                                                               #\n",
    "    ############################################################################################################################\n",
    "    scaler = StandardScaler()\n",
    "    model_xgb1 = xgb.XGBClassifier(random_state=2000, n_jobs=6, reg_lambda=0, learning_rate=0.2, objective='multi:softmax',\n",
    "                           max_depth=5, subsample=0.8, colsample_bytree=0.8, seed=100)\n",
    "    pipe_XGB = make_pipeline(scaler, model_xgb1)\n",
    "\n",
    "    pipe_steps_XGB = list(pipe_XGB.named_steps.keys())\n",
    "    print('Pipeline steps:', pipe_steps_XGB)\n",
    "    model_step_XGB = pipe_steps_XGB[1]\n",
    "\n",
    "    hp_space1_XGB = {\n",
    "        model_step_XGB + '__max_depth':range(3,10,2),\n",
    "        model_step_XGB + '__min_child_weight':range(1,6,2),\n",
    "        model_step_XGB + '__num_class': [2]\n",
    "    } \n",
    "    gscv_XGB1 = GridSearchCV(\n",
    "        estimator=pipe_XGB, param_grid=hp_space1_XGB, cv=5, scoring='f1_micro', n_jobs=-1, return_train_score=True, verbose=2\n",
    "    )\n",
    "\n",
    "    ############################################################################################################################\n",
    "    #                                              STACKING                                                                    #\n",
    "    ############################################################################################################################\n",
    "    # Initializing Support Vector classifier\n",
    "    classifier1 = SVC(C = 50, degree = 1, gamma = \"auto\", kernel = \"rbf\", probability = True)\n",
    "    # Initializing Multi-layer perceptron  classifier\n",
    "    classifier2 = MLPClassifier(activation = \"relu\", alpha = 0.1, hidden_layer_sizes = (10,10,10),\n",
    "                                learning_rate = \"constant\", max_iter = 2000, random_state = 1000)\n",
    "    # Initialing Nu Support Vector classifier\n",
    "    classifier3 = NuSVC(degree = 1, kernel = \"rbf\", nu = 0.25, probability = True)\n",
    "    # Initializing Random Forest classifier\n",
    "    classifier4 = RandomForestClassifier(n_estimators = 500, criterion = \"gini\", max_depth = 10,\n",
    "                                          max_features = \"auto\", min_samples_leaf = 0.005,\n",
    "                                          min_samples_split = 0.005, n_jobs = -1, random_state = 1000)\n",
    "    # Initializing the StackingCV classifier\n",
    "    sclf = StackingCVClassifier(classifiers = [classifier1, classifier2, classifier3, classifier4],\n",
    "                                shuffle = False,\n",
    "                                use_probas = True,\n",
    "                                cv = 5,\n",
    "                                meta_classifier = SVC(probability = True))\n",
    "    model_step_Stacking = 'meta_classifier'\n",
    "    params = {\n",
    "        model_step_Stacking + \"__kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "        model_step_Stacking + \"__C\": [1, 2],\n",
    "        model_step_Stacking + \"__degree\": [3, 4, 5],\n",
    "        model_step_Stacking + \"__probability\": [True]\n",
    "    }\n",
    "    gscvStacking1 = GridSearchCV(\n",
    "        estimator = sclf, \n",
    "        param_grid = params, \n",
    "        cv = 5, scoring = \"roc_auc\", verbose = 10, n_jobs = -1)\n",
    "\n",
    "    ############################################################################################################################\n",
    "    #                                              Multiple SVM Models                                                         #\n",
    "    ############################################################################################################################\n",
    "    scaler = MinMaxScaler()\n",
    "    pca = PCA()\n",
    "    model_MultipleSVM = SVC(class_weight = weight_code)\n",
    "    pipe_MultipleSVM = make_pipeline(scaler, pca, model_MultipleSVM)\n",
    "\n",
    "    pipe_steps_MultipleSVM = list(pipe_MultipleSVM.named_steps.keys())\n",
    "    print('Pipeline steps:', pipe_steps_MultipleSVM)\n",
    "    model_step_PCA = pipe_steps_SVM[1]\n",
    "    model_step_MultipleSVM = pipe_steps_MultipleSVM[2]\n",
    "    parameters_MultipleSVM = {\n",
    "        model_step_PCA + '__n_components': [8],\n",
    "        model_step_MultipleSVM + \"__kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "        model_step_MultipleSVM + \"__C\": [1, 2],\n",
    "        model_step_MultipleSVM + \"__degree\": [3, 4, 5],\n",
    "        model_step_MultipleSVM + \"__probability\": [True]\n",
    "    }\n",
    "\n",
    "    gscv_MultipleSVM = GridSearchCV(\n",
    "        estimator=pipe_MultipleSVM, param_grid=parameters_SVM, scoring=\"accuracy\", cv=5,\n",
    "        n_jobs=-1, verbose=2, return_train_score=True\n",
    "    )\n",
    "\n",
    "    model_SVM = SVC(kernel=\"rbf\", class_weight = weight_code)\n",
    "    pipe_SVM = make_pipeline(scaler, pca, model_SVM)\n",
    "\n",
    "    ############################################################################################################################\n",
    "    #                                              Simple Models                                                               #\n",
    "    ############################################################################################################################\n",
    "    if (model_type == 'Original' and class_weight == 'None'): \n",
    "        return {\n",
    "            # \"Logistic Regression\"        : [LogisticRegression(),''], \n",
    "            # \"Decision Tree\"              : [DecisionTreeClassifier(),''], \n",
    "            # \"Random Forest\"              : [RandomForestClassifier(),''], \n",
    "            # \"Bernoulli NB\"               : [BernoulliNB(),''], \n",
    "            # \"Multinomial NB\"             : [MultinomialNB(),''], \n",
    "            # \"Gaussian NB\"                : [GaussianNB(),''],\n",
    "            # \"SVM (Linear)\"               : [LinearSVC(),''], \n",
    "            # \"K-Nearest Neighbors\"        : [KNeighborsClassifier(),''],\n",
    "            # \"Stochastic Gradient Descent\": [SGDClassifier(),'']\n",
    "        }\n",
    "    \n",
    "    if (model_type == 'Original' and class_weight == 'Balanced'): \n",
    "        return {\n",
    "            \"Logistic Regression\":         [LogisticRegression(class_weight='balanced'),''], \n",
    "            \"Decision Tree\":               [DecisionTreeClassifier(class_weight='balanced'),''], \n",
    "            \"Random Forest\":               [RandomForestClassifier(class_weight='balanced'),''], \n",
    "            \"SVM (Linear)\":                [LinearSVC(class_weight='balanced'),''], \n",
    "            \"Stochastic Gradient Descent\": [SGDClassifier(class_weight='balanced'),'']\n",
    "        }\n",
    "\n",
    "    ############################################################################################################################\n",
    "    #                                              MULTIPLE MODELS                                                             #\n",
    "    ############################################################################################################################\n",
    "    if (model_type == 'Pipeline' and class_weight == 'None'): \n",
    "        return {\n",
    "            # 'SVM (Linear)': [gscv_SVC, model_step_SVC],\n",
    "            # 'Logistic regression': [gscv_LR, model_step_LR],\n",
    "            # 'Random Forest RandCV': [randcv, model_step_RF],\n",
    "            # 'Stochastic Gradient Descent': [gscv_SGD, model_step_SGD],\n",
    "            # 'Random Forest GridCV': [gscv2, model_step_RF], \n",
    "            # 'Xtreme Gradient Boosting': [gscv_XGB1, model_step_XGB],\n",
    "            # 'SVM (Radial Basis Kernel)': [gscv_SVM, model_step_SVM],\n",
    "            # 'Multiple SVM': [gscv_MultipleSVM, model_step_MultipleSVM],\n",
    "            'Ensemble model Stacking': [gscvStacking1, model_step_Stacking]\n",
    "        }\n",
    "    if (model_type == 'Pipeline') & (class_weight == 'Balanced'):\n",
    "        return {\n",
    "            'SVM (Linear)': [gscv_SVC, model_step_SVC],\n",
    "            'Logistic regression': [gscv_LR, model_step_LR],\n",
    "            'Random Forest RandCV': [randcv, model_step_RF],\n",
    "            'Stochastic Gradient Descent': [gscv_SGD, model_step_SGD],\n",
    "            'Random Forest GridCV': [gscv2, model_step_RF], \n",
    "            'SVM (Radial Basis Kernel)': [gscv_SVM, model_step_SVM],\n",
    "            'Multiple SVM': [gscv_MultipleSVM, model_step_MultipleSVM],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Gk6BOj_ss8a"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, auc\n",
    "\n",
    "# -------------------------------------------------train_models_pipe ----------------------------------------------------------#\n",
    "\n",
    "def train_models(model, model_step, model_name, model_type, meta_data, class_weight):\n",
    "    scores_origin = []\n",
    "    for key, value in meta_data.items(): \n",
    "        description = key\n",
    "        X_train_sub = meta_data[key][0]\n",
    "        y_train_sub = meta_data[key][1]\n",
    "        print(\"----------Dataset: \" + key + \" with length: \" + str(X_train_sub.__len__()))\n",
    "        try: \n",
    "            model.fit(X_train_sub, y_train_sub)\n",
    "        except: \n",
    "            print('Dataset is a DataFrame')\n",
    "            model.fit(X_train_sub.values, y_train_sub.values)\n",
    "        # define paths to store values\n",
    "        ROOT_PATH_MODEL = os.path.join(ROOT_PATH_MODELS, model_name)\n",
    "        ROOT_PATH_CSV   = ROOT_PATH_MODELS\n",
    "        model_path      = os.path.join(ROOT_PATH_MODEL, version_code + class_weight + '_' + model_name \n",
    "                                      + '.sav')\n",
    "        model_path_desc = os.path.join(ROOT_PATH_MODEL, version_code + class_weight + '_' + model_name + '_' \n",
    "                                      + description + '.sav')\n",
    "        testing_record_path = os.path.join (ROOT_PATH_CSV, 'Testing Record', \n",
    "                                            version_code + class_weight + '_' + model_name + '_' + description  \n",
    "                                            + '_testing_record.csv')\n",
    "        training_result_path = os.path.join(ROOT_PATH_CSV, 'Training Record', \n",
    "                                            version_code + class_weight + '_' + model_name + '_' + description  \n",
    "                                            + '_training_record.csv')\n",
    "        csv_path = os.path.join(ROOT_PATH_CSV, version_code + class_weight + \"_\" + description + \"_\" + model_type + \n",
    "                                '_' + model_name + '.csv')\n",
    "        if model_type == 'Original': \n",
    "            # Append the current model\n",
    "            scores_origin.append(evaluate_models(\n",
    "                model, model_name, model_step, model_path, model_type, testing_record_path, model_path_desc, X_test, y_test, \n",
    "                description, class_weight))\n",
    "        else: \n",
    "            # Output the training results\n",
    "            training_results = pd.DataFrame(model.cv_results_)\n",
    "            training_results.to_csv(training_result_path, index=False)\n",
    "            # Append the current model\n",
    "            scores_origin.append(evaluate_models(\n",
    "                model, model_name, model_step, model_path, model_type, testing_record_path, \n",
    "                model_path_desc, X_test_ori, y_test_ori, \n",
    "                description, class_weight))\n",
    "        print(\"---------------Updating\\n\")\n",
    "        df = pd.DataFrame(scores_origin)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "    # always try to update the newest results\n",
    "    df_origin = pd.DataFrame(scores_origin)\n",
    "    return df_origin, model, model_name, description\n",
    "\n",
    "# -------------------------------------------------evaluate_model_pipe --------------------------------------------------------#\n",
    "\n",
    "def evaluate_models(estimator, model_name, model_step, model_path, model_type, \n",
    "                        testing_record_path, model_path_desc, X, y, description, class_weight):\n",
    "    begin = True\n",
    "    # Stacking models we use probability to predict\n",
    "    print(\"predict values\")\n",
    "    prediction = estimator.predict(X)\n",
    "\n",
    "    # Set print options\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # model_name = type(estimator).__name__\n",
    "    current_score = f1_score(y, prediction, average='micro')\n",
    "\n",
    "    score = []\n",
    "    if model_step != '':\n",
    "        current_model = estimator.best_estimator_\n",
    "        # save current model\n",
    "        save_model(model_path_desc, current_model)\n",
    "        # write down current report model\n",
    "        test_record_data = {}\n",
    "        result = {\n",
    "            'Name': model_name, \n",
    "            'F1_score': current_score,\n",
    "            'Recall': recall_score(y, prediction),\n",
    "            'Precision': precision_score(y, prediction),\n",
    "            'Accuracy': accuracy_score(y, prediction),\n",
    "            'AUC': roc_auc_score(y, prediction),\n",
    "            'Confusion Matrix': confusion_matrix(y, prediction),\n",
    "            'Description': description,\n",
    "            'Remarks': 'Pipeline',\n",
    "            'Class_weight': class_weight,\n",
    "            'Current model': estimator.best_estimator_,\n",
    "            'Best parameters': estimator.best_params_,\n",
    "            'Best score': estimator.best_score_,\n",
    "            'Training Time': estimator.refit_time_,\n",
    "            'Training Score': estimator.best_score_,\n",
    "        }\n",
    "        score.append(result)\n",
    "        score_df = pd.DataFrame(score)\n",
    "        score_df.to_csv(model_name + \"_Result_temp.csv\", index=False)\n",
    "        try: \n",
    "            test_record_data = current_model.named_steps[model_step].get_params().copy()\n",
    "        except: \n",
    "            print(\"Name_steps not in estimator parameters\")\n",
    "        test_record_data[\"training_time\"] = estimator.refit_time_\n",
    "        test_record_data[\"training_score\"] = estimator.best_score_\n",
    "        test_record_data[\"f1_testing_score\"] = current_score\n",
    "        write_down_records(test_record_data, testing_record_path)\n",
    "        if begin: \n",
    "            save_model(model_path, current_model)\n",
    "            begin = False\n",
    "        # note the best_model so far\n",
    "        note_best_model(model_path, current_model, current_score, test_record_data, testing_record_path)\n",
    "    else: \n",
    "        result = {\n",
    "            'Name': model_name, \n",
    "            'F1_score': current_score,\n",
    "            'Recall': recall_score(y, prediction),\n",
    "            'Precision': precision_score(y, prediction),\n",
    "            'Accuracy': accuracy_score(y, prediction),\n",
    "            'AUC': roc_auc_score(y, prediction),\n",
    "            'Confusion Matrix': confusion_matrix(y, prediction),\n",
    "            'Description': description,\n",
    "            'Remarks': 'Original',\n",
    "            'Class_weight': class_weight\n",
    "        }\n",
    "        score.append(result)\n",
    "        score_df = pd.DataFrame(score)\n",
    "        score_df.to_csv(model_name + \"_Result_temp.csv\", index=False)\n",
    "    return result\n",
    "\n",
    "# -------------------------------------------------save_model -----------------------------------------------------------------#\n",
    "\n",
    "def save_model(model_path, current_model):\n",
    "    if os.path.isfile(model_path):\n",
    "        os.remove(model_path)\n",
    "    pickle.dump(current_model, open(model_path, \"wb\"))\n",
    "\n",
    "# -------------------------------------------------write_down_records ---------------------------------------------------------#\n",
    "\n",
    "def write_down_records(test_record_data, testing_record_path): \n",
    "    fieldnames = list(test_record_data.keys())\n",
    "    with open(testing_record_path, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerow(test_record_data)\n",
    "        \n",
    "# -------------------------------------------------note_best_model ------------------------------------------------------------#\n",
    "\n",
    "def note_best_model(model_path, current_model, current_score, test_record_data, testing_record_path): \n",
    "    best_model = pickle.load(open(model_path, \"rb\"))\n",
    "    best_preds = best_model.predict(X_test)\n",
    "    best_score = f1_score(y_test, best_preds, average='micro')\n",
    "\n",
    "    if current_score > best_score:\n",
    "        pickle.dump(current_model, open(model_path, \"wb\"))\n",
    "        fieldnames = list(test_record_data.keys())\n",
    "        with open(testing_record_path, 'a', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writerow(test_record_data)\n",
    "        best_model = current_model\n",
    "        best_preds = current_preds\n",
    "        best_score = current_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Cg6PFOMss8g"
   },
   "source": [
    "## Train and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "726JESYelmBe"
   },
   "outputs": [],
   "source": [
    "trained_classifier = {}\n",
    "def train_all_models(models_list_pipeline, model_type, class_weight):\n",
    "    csv_path = os.path.join(ROOT_PATH_MODELS, version_code + class_weight + '_' model_tye + \"_\" + 'models.csv')\n",
    "    data_df = {}\n",
    "    if (model_type == 'Original'):\n",
    "        data_list = create_meta_data(X_train, y_train)\n",
    "    else: \n",
    "        data_list = create_meta_data(X_train_ori, y_train_ori)\n",
    "    if class_weight == 'Balanced': # Balanced model needs to train Origin Data only\n",
    "        data_df['Origin'] = data_list['Origin']\n",
    "    else: \n",
    "        data_df = data_list\n",
    "    if not os.path.isfile(csv_path):\n",
    "        meta_model_pipe = pd.DataFrame()\n",
    "    else: \n",
    "        meta_model_pipe = pd.read_csv(csv_path)\n",
    "    for model_name, model_arr in models_list_pipeline.items():\n",
    "        print(\"----Model Name: \" + model_name)\n",
    "        \n",
    "        # Fit the model\n",
    "        print(\"--------Fitting Model !!!\")\n",
    "        model = model_arr[0]\n",
    "        model_step = model_arr[1]\n",
    "\n",
    "        # use a copy of meta_data\n",
    "        model_DF, classifier, model_name, description = train_models(model, model_step, model_name, model_type,\n",
    "                                                                     data_df, class_weight)\n",
    "        meta_model_pipe = pd.concat([meta_model_pipe, model_DF], ignore_index=True)\n",
    "        print(\"===============PRODUCING CSV FILE FOR MODEL \" + model_name + \"\\n\\n\")\n",
    "        meta_model_pipe.sort_values(['F1_score'], ascending=False, inplace=True)\n",
    "        meta_model_pipe.to_csv(csv_path, index=False)\n",
    "\n",
    "        trained_classifier[str(model_name + description + class_weight)] = [classifier, description, class_weight]\n",
    "    return meta_model_pipe\n",
    "\n",
    "# -------------------------------------------------train_models ---------------------------------------------------------------#\n",
    "def train_meta_models(model_option): \n",
    "    data_df = pd.DataFrame()\n",
    "    for ele in model_option: \n",
    "        class_weight = ele[0]\n",
    "        model_type = ele[1]\n",
    "        model_lists = get_all_models_lists(model_type, class_weight)\n",
    "        data = train_all_models(model_lists, model_type, class_weight)\n",
    "        data_df = pd.concat([data_df, data], index=False)\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dOM8Qn0uss8g",
    "outputId": "e50a8dae-2b15-4bbb-8a5b-9551e140db10",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_meta_models(model_option)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Nguyen_Ngoc_Linh_Chi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
